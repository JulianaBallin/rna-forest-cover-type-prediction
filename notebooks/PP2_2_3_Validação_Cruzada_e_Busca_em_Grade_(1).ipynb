{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVPf2qACAJBU"
      },
      "source": [
        "# Redes Neurais Artificiais 2025.2\n",
        "\n",
        "- **Disciplina**: Redes Neurais Artificiais 2025.2\n",
        "- **Professora**: Elloá B. Guedes (ebgcosta@uea.edu.br)  \n",
        "- **Github**: http://github.com/elloa  \n",
        "        \n",
        "\n",
        "Levando em conta a base de dados **_Forest Cover Type_**, esta parte do Projeto Prático diz respeito à proposição e avaliação de múltiplas redes neurais artificiais do tipo feedforward multilayer perceptron para o problema da classificação multi-classe da cobertura florestal em uma área do Roosevelt National Forest.\n",
        "\n",
        "## Busca em Grade\n",
        "\n",
        "Uma maneira padrão de escolher os parâmetros de um modelo de Machine Learning é por meio de uma busca em grade via força bruta. O algoritmo da busca em grade é dado como segue:\n",
        "\n",
        "1. Escolha a métrica de desempenho que você deseja maximizar  \n",
        "2. Escolha o algoritmo de Machine Learning (exemplo: redes neurais artificiais). Em seguida, defina os parâmetros ou hiperparâmetros deste tipo de modelo sobre os quais você dseja otimizar (número de épocas, taxa de aprendizado, etc.) e construa um array de valores a serem testados para cada parâmetro ou hiperparâmetro.  \n",
        "3. Defina a grade de busca, a qual é dada como o produto cartesiano de cada parâmetro a ser testado. Por exemplo, para os arrays [50, 100, 1000] e [10, 15], tem-se que a grade é [(50,10), (50,15), (100,10), (100,15), (1000,10), (1000,15)].\n",
        "4. Para cada combinação de parâmetros a serem otimizados, utilize o conjunto de treinamento para realizar uma validação cruzada (holdout ou k-fold) e calcule a métrica de avaliação no conjunto de teste (ou conjuntos de teste)\n",
        "5. Escolha a combinação de parâmetros que maximizam a métrica de avaliação. Este é o modelo otimizado.\n",
        "\n",
        "Por que esta abordagem funciona? Porque a busca em grade efetua uma pesquisa extensiva sobre as possíveis combinações de valores para cada um dos parâmetros a serem ajustados. Para cada combinação, ela estima a performance do modelo em dados novos. Por fim, o modelo com melhor métrica de desempenho é escolhido. Tem-se então que este modelo é o que melhor pode vir a generalizar mediante dados nunca antes vistos.\n",
        "\n",
        "## Efetuando a Busca em Grade sobre Hiperparâmetros das Top-6 RNAs\n",
        "\n",
        "Considerando a etapa anterior do projeto prático, foram identificadas pelo menos 6 melhores Redes Neurais para o problema da classificação multi-classe da cobertura florestal no conjunto de dados selecionado. Algumas destas redes possuem atributos categóricos como variáveis preditoras, enquanto outras possuem apenas os atributos numéricos como preditores.\n",
        "\n",
        "A primeira etapa desta segunda parte do projeto consiste em trazer para este notebook estas seis arquiteturas, ressaltando:\n",
        "\n",
        "1. Número de neurônios ocultos por camada  \n",
        "2. Função de Ativação  \n",
        "3. Utilização ou não de atributos categóricos   \n",
        "4. Desempenho médio +- desvio padrão nos testes anteriores  \n",
        "5. Número de repetições que a equipe conseguiu realizar para verificar os resultados  \n",
        "\n",
        "Elabore uma busca em grade sobre estas arquiteturas que contemple variações nos hiperparâmetros a seguir, conforme documentação de [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
        "\n",
        "A. Solver  (Não usar o LBFGS, pois é mais adequado para datasets pequenos)  \n",
        "B. Batch Size  \n",
        "C. Learning Rate Init  \n",
        "D. Paciência (n_iter_no_change)  \n",
        "E. Épocas  \n",
        "\n",
        "Nesta busca em grande, contemple a utilização do objeto [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYN5DbptAJBW"
      },
      "source": [
        "## Validação Cruzada k-fold\n",
        "\n",
        "Na elaboração da busca em grid, vamos avaliar os modelos propostos segundo uma estratégia de validação cruzada ainda não explorada até o momento: a validação cruzada k-fold. Segundo a mesma, o conjunto de dados é particionado em k partes: a cada iteração, separa-se uma das partes para teste e o modelo é treinado com as k-1 partes remanescentes. Valores sugestivos de k na literatura são k = 3, 5 ou 10, pois o custo computacional desta validação dos modelos é alto. A métrica de desempenho é resultante da média dos desempenhos nas k iterações. A figura a seguir ilustra a ideia desta avaliação\n",
        "\n",
        "<img src = \"https://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png\" width=600></img>\n",
        "\n",
        "Considerando a métrica de desempenho F1-Score, considere a validação cruzada 5-fold para aferir os resultados da busca em grande anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIwpixvWH8iN"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas numpy matplotlib seaborn torch torchvision torchaudio kagglehub ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gjrfZklAJBY"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Baixar o dataset do Kaggle\n",
        "path = kagglehub.dataset_download(\"uciml/forest-cover-type-dataset\")\n",
        "print(\"Dataset baixado em:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46Bojm2PAJBZ"
      },
      "outputs": [],
      "source": [
        "# Cell A — locate dataset in Colab cache\n",
        "import os, pprint, glob\n",
        "candidates = glob.glob('/kaggle/input/forest-cover-type-dataset/**', recursive=True)\n",
        "files = [p for p in candidates if os.path.isfile(p)]\n",
        "print(\"Some files in /kaggle/input/forest-cover-type-dataset:\")\n",
        "pprint.pprint(files[:20])\n",
        "# Common CSV path:\n",
        "CSV_PATH = '/kaggle/input/forest-cover-type-dataset/covtype.csv'\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    # fallback: find any covtype*.csv\n",
        "    import glob\n",
        "    found = glob.glob('/kaggle/input/**/covtype*.csv', recursive=True)\n",
        "    if found:\n",
        "        CSV_PATH = found[0]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"covtype.csv not found in /kaggle/input; check files list above.\")\n",
        "print(\"Using CSV_PATH =\", CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0sKFvJQAJBa"
      },
      "outputs": [],
      "source": [
        "# Cell B — load CSV and quick inspect\n",
        "import pandas as pd\n",
        "CSV_PATH = '/kaggle/input/forest-cover-type-dataset/covtype.csv'  # adjust if needed\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Loaded df shape:\", df.shape)\n",
        "display(df.head())\n",
        "print(\"Columns (first 20):\", df.columns.tolist()[:20])\n",
        "# configure sample size: None to use all data (careful!), or integer to subsample stratified\n",
        "MAX_SAMPLES = None   # safe default for Colab; set to None to use full dataset if you have time/ram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e9hB6ixAJBb"
      },
      "outputs": [],
      "source": [
        "# Cell C — prepare X, y and optional stratified sampling\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "target_col = 'Cover_Type' if 'Cover_Type' in df.columns else df.columns[-1]\n",
        "print(\"Target column:\", target_col)\n",
        "X = df.drop(columns=[target_col]).values.astype(np.float32)\n",
        "y = df[target_col].values.astype(np.int64)\n",
        "# convert labels from 1..7 to 0..6 if needed\n",
        "if y.min() == 1:\n",
        "    y = y - 1\n",
        "\n",
        "print(\"Original dataset size:\", X.shape)\n",
        "\n",
        "if MAX_SAMPLES is not None and X.shape[0] > MAX_SAMPLES:\n",
        "    print(f\"Stratified sampling to {MAX_SAMPLES} samples...\")\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=MAX_SAMPLES, random_state=42)\n",
        "    for _, idx in sss.split(X, y):\n",
        "        X_sample = X[idx]\n",
        "        y_sample = y[idx]\n",
        "else:\n",
        "    X_sample, y_sample = X, y\n",
        "\n",
        "print(\"Using sample shape:\", X_sample.shape, \"n_classes:\", len(np.unique(y_sample)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM9goNh9Hxee"
      },
      "outputs": [],
      "source": [
        "# Cell D — PyTorch helpers\n",
        "import torch, gc\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_classes, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, max(hidden_dim//2, 8)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(max(hidden_dim//2, 8), n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            ps.append(preds)\n",
        "            ys.append(yb.numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    return float(f1_score(y_true, y_pred, average='weighted')), y_true, y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kO9QDi7YKe-"
      },
      "outputs": [],
      "source": [
        "# Cell E — grid search manual (small grid); adjust param_grid for more experiments\n",
        "import time, itertools, json\n",
        "from copy import deepcopy\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "\n",
        "n_classes = len(np.unique(y_sample))\n",
        "input_dim = X_sample.shape[1]\n",
        "print(\"input_dim:\", input_dim, \"n_classes:\", n_classes)\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_dim': [128, 256],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'batch_size': [256],\n",
        "    'epochs': [8]\n",
        "}\n",
        "\n",
        "def iter_grid(grid):\n",
        "    keys = list(grid.keys())\n",
        "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
        "        yield dict(zip(keys, vals))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_cfg, best_score = None, -1.0\n",
        "results = []\n",
        "\n",
        "for cfg in iter_grid(param_grid):\n",
        "    fold_scores = []\n",
        "    t0 = time.time()\n",
        "    print(\"Testing cfg:\", cfg)\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_sample, y_sample), 1):\n",
        "        # fit scaler on train fold\n",
        "        scaler = StandardScaler()\n",
        "        X_train_fold = scaler.fit_transform(X_sample[train_idx])\n",
        "        X_val_fold = scaler.transform(X_sample[val_idx])\n",
        "        y_train_fold = y_sample[train_idx]\n",
        "        y_val_fold = y_sample[val_idx]\n",
        "\n",
        "        train_ds = NumpyDataset(X_train_fold, y_train_fold)\n",
        "        val_ds = NumpyDataset(X_val_fold, y_val_fold)\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False)\n",
        "\n",
        "        model = MLP(input_dim=input_dim, hidden_dim=cfg['hidden_dim'], n_classes=n_classes).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=cfg['lr'])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for ep in range(cfg['epochs']):\n",
        "            _ = train_one_epoch(model, train_loader, opt, criterion)\n",
        "\n",
        "        f1_val, _, _ = eval_model(model, val_loader)\n",
        "        fold_scores.append(f1_val)\n",
        "\n",
        "        # cleanup\n",
        "        del model, opt, criterion, train_loader, val_loader, train_ds, val_ds\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    mean_f1 = float(np.mean(fold_scores))\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"  Mean F1 5-fold = {mean_f1:.4f} (time {elapsed:.1f}s)\")\n",
        "    results.append((cfg, mean_f1))\n",
        "    if mean_f1 > best_score:\n",
        "        best_score = mean_f1\n",
        "        best_cfg = deepcopy(cfg)\n",
        "\n",
        "print(\"\\nBest config:\", best_cfg, \"best_cv_f1 =\", best_score)\n",
        "# save results\n",
        "with open('/content/grid_search_results.json', 'w') as f:\n",
        "    json.dump({'best': best_cfg, 'best_score': best_score, 'results': [[r[0], r[1]] for r in results]}, f, indent=2)\n",
        "print(\"Saved grid search summary to /content/grid_search_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA7c50uTYQPf"
      },
      "outputs": [],
      "source": [
        "# Cell F — train final on X_sample with scaler + save model & scaler\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "best = best_cfg\n",
        "print(\"Training final with:\", best)\n",
        "\n",
        "scaler_final = StandardScaler()\n",
        "X_scaled = scaler_final.fit_transform(X_sample)\n",
        "y_final = y_sample\n",
        "\n",
        "final_ds = NumpyDataset(X_scaled, y_final)\n",
        "final_loader = DataLoader(final_ds, batch_size=best['batch_size'], shuffle=True)\n",
        "\n",
        "model_final = MLP(input_dim=input_dim, hidden_dim=best['hidden_dim'], n_classes=n_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model_final.parameters(), lr=best['lr'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(best['epochs']):\n",
        "    loss = train_one_epoch(model_final, final_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}/{best['epochs']} - loss: {loss:.4f}\")\n",
        "\n",
        "MODEL_PATH = '/content/cover_mlp_best.pth'\n",
        "SCALER_PATH = '/content/cover_scaler.pkl'\n",
        "torch.save(model_final.state_dict(), MODEL_PATH)\n",
        "joblib.dump(scaler_final, SCALER_PATH)\n",
        "print(\"Saved model to\", MODEL_PATH)\n",
        "print(\"Saved scaler to\", SCALER_PATH)\n",
        "\n",
        "# save summary\n",
        "summary = {\n",
        "    'n_total_available': int(X.shape[0]),\n",
        "    'n_used_sample': int(X_sample.shape[0]),\n",
        "    'best_cfg': best,\n",
        "    'best_cv_f1': best_score,\n",
        "    'model_path': MODEL_PATH,\n",
        "    'scaler_path': SCALER_PATH\n",
        "}\n",
        "import json\n",
        "with open('/content/cover_training_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Saved training summary to /content/cover_training_summary.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axxjZKd_AJBb"
      },
      "source": [
        "## Identificando a mellhor solução\n",
        "\n",
        "Como resultado da busca em grande com validação cruzada 5-fold, identifique o modelo otimizado com melhor desempenho para o problema. Apresente claramente este modelo, seus parâmetros, hiperparâmetros otimizados e resultados para cada um dos folds avaliados. Esta é a melhor solução identificada em decorrência deste projeto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56vl-eZyAJBc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json, time, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try: torch.set_float32_matmul_precision(\"high\")\n",
        "except: pass\n",
        "\n",
        "# Descobre X/y vindos do notebook; senão usa covertype da sklearn\n",
        "def _pick_xy():\n",
        "    for Xn, yn in [(\"Xcv\",\"ycv\"), (\"X_use\",\"y_use\"), (\"X\",\"y\")]:\n",
        "        if Xn in globals() and yn in globals():\n",
        "            X_, y_ = globals()[Xn], globals()[yn]\n",
        "            return np.asarray(X_, dtype=np.float32), np.asarray(y_, dtype=int)\n",
        "    # fallback: covertype\n",
        "    from sklearn.datasets import fetch_covtype\n",
        "    df = fetch_covtype(as_frame=True).frame\n",
        "    y_ = df[\"Cover_Type\"].astype(int).to_numpy()\n",
        "    X_ = df.drop(columns=[\"Cover_Type\"]).to_numpy(dtype=np.float32)\n",
        "\n",
        "    if X_.shape[0] > 60000:\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_, _, y_, _ = train_test_split(X_, y_, train_size=60000, stratify=y_, random_state=42)\n",
        "    return X_.astype(np.float32), y_\n",
        "\n",
        "X_all, y_all = _pick_xy()\n",
        "n_features = X_all.shape[1]; n_classes = int(np.max(y_all))+1\n",
        "print(f\"Dados: X={X_all.shape}, y={y_all.shape}, classes={n_classes}, device={DEVICE}\")\n",
        "\n",
        "# Modelo MLP simples\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_dim\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            d = h\n",
        "        layers += [nn.Linear(d, out_dim)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "def train_one_fold(X_tr, y_tr, X_va, y_va, config, max_epochs=200, patience=15, bs=2048):\n",
        "    # scaler por fold\n",
        "    scaler = StandardScaler().fit(X_tr)\n",
        "    Xt = scaler.transform(X_tr).astype(np.float32)\n",
        "    Xv = scaler.transform(X_va).astype(np.float32)\n",
        "\n",
        "    tl = DataLoader(TensorDataset(torch.from_numpy(Xt), torch.from_numpy(y_tr)), batch_size=bs, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    vl = DataLoader(TensorDataset(torch.from_numpy(Xv), torch.from_numpy(y_va)), batch_size=bs, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = MLP(n_features, config[\"hidden\"], n_classes, dropout=config.get(\"dropout\",0.2)).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config.get(\"wd\",1e-4))\n",
        "    scaler_amp = GradScaler(enabled=(DEVICE.type==\"cuda\"))\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1, best_state, noimp = -1.0, None, 0\n",
        "    for ep in range(1, max_epochs+1):\n",
        "        model.train()\n",
        "        for xb, yb in tl:\n",
        "            xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(DEVICE.type==\"cuda\")):\n",
        "                loss = crit(model(xb), yb)\n",
        "            scaler_amp.scale(loss).backward(); scaler_amp.step(opt); scaler_amp.update()\n",
        "\n",
        "        # valida\n",
        "        model.eval(); preds, gts = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl:\n",
        "                xb = xb.to(DEVICE, non_blocking=True)\n",
        "                with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(DEVICE.type==\"cuda\")):\n",
        "                    p = model(xb).argmax(1).cpu()\n",
        "                preds.append(p); gts.append(yb)\n",
        "        yv = torch.cat(gts).numpy(); pv = torch.cat(preds).numpy()\n",
        "        f1 = f1_score(yv, pv, average=\"macro\")\n",
        "\n",
        "        if f1 > best_f1 + 1e-4:\n",
        "            best_f1, noimp = f1, 0\n",
        "            best_state = {k: v.cpu() for k,v in model.state_dict().items()}\n",
        "        else:\n",
        "            noimp += 1\n",
        "            if noimp >= patience: break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return best_f1, model.cpu(), scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E9SBFNFAJBd"
      },
      "outputs": [],
      "source": [
        "import time, json, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "from contextlib import nullcontext\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_CUDA = (DEVICE.type == \"cuda\")\n",
        "\n",
        "try:\n",
        "    from torch import amp\n",
        "    def AMP():\n",
        "        return amp.autocast(\"cuda\", dtype=torch.float16) if USE_CUDA else nullcontext()\n",
        "    def make_scaler():\n",
        "        return amp.GradScaler(\"cuda\") if USE_CUDA else None\n",
        "    print(\"AMP: usando torch.amp ✓\")\n",
        "except Exception:\n",
        "    from torch.cuda.amp import autocast as legacy_autocast, GradScaler as LegacyGradScaler\n",
        "    def AMP():\n",
        "        return legacy_autocast(enabled=USE_CUDA, dtype=torch.float16)\n",
        "    def make_scaler():\n",
        "        return LegacyGradScaler(enabled=USE_CUDA)\n",
        "    print(\"AMP: usando torch.cuda.amp (legacy) ✓\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try: torch.set_float32_matmul_precision(\"high\")\n",
        "except: pass\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "assert \"X_all\" in globals() and \"y_all\" in globals(), \"Cadê X_all/y_all? Rode a célula G1 antes.\"\n",
        "assert \"n_features\" in globals() and \"n_classes\" in globals(), \"Cadê n_features/n_classes? Rode a célula G1 antes.\"\n",
        "assert \"MLP\" in globals(), \"Cadê a classe MLP? Ela é definida na G1.\"\n",
        "\n",
        "# função de treino\n",
        "def train_one_fold(X_tr, y_tr, X_va, y_va, config, max_epochs=120, patience=12, bs=2048):\n",
        "    scaler = StandardScaler().fit(X_tr)\n",
        "    Xt = scaler.transform(X_tr).astype(np.float32)\n",
        "    Xv = scaler.transform(X_va).astype(np.float32)\n",
        "\n",
        "    tl = DataLoader(TensorDataset(torch.from_numpy(Xt), torch.from_numpy(y_tr)), batch_size=bs, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    vl = DataLoader(TensorDataset(torch.from_numpy(Xv), torch.from_numpy(y_va)), batch_size=bs, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = MLP(n_features, config[\"hidden\"], n_classes, dropout=config.get(\"dropout\", 0.2)).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config.get(\"wd\", 1e-4))\n",
        "    scaler_amp = make_scaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1, noimp, best_state = -1.0, 0, None\n",
        "    for ep in range(1, max_epochs + 1):\n",
        "        # treino\n",
        "        model.train()\n",
        "        for xb, yb in tl:\n",
        "            xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with AMP():\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "            if scaler_amp is not None:\n",
        "                scaler_amp.scale(loss).backward()\n",
        "                scaler_amp.step(opt)\n",
        "                scaler_amp.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # validação\n",
        "        model.eval(); preds, gts = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in vl:\n",
        "                xb = xb.to(DEVICE, non_blocking=True)\n",
        "                with AMP():\n",
        "                    p = model(xb).argmax(1).cpu()\n",
        "                preds.append(p); gts.append(yb)\n",
        "        yv = torch.cat(gts).numpy(); pv = torch.cat(preds).numpy()\n",
        "        f1 = f1_score(yv, pv, average=\"macro\")\n",
        "\n",
        "        if f1 > best_f1 + 1e-4:\n",
        "            best_f1, noimp = f1, 0\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            noimp += 1\n",
        "            if noimp >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return best_f1, model.cpu(), scaler\n",
        "\n",
        "# =====================================================================\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "param_grid = [\n",
        "    {\"name\":\"mlp_128_lr1e-3\", \"hidden\":[128], \"lr\":1e-3},\n",
        "    {\"name\":\"mlp_128_lr5e-4\", \"hidden\":[128], \"lr\":5e-4},\n",
        "    {\"name\":\"mlp_256_lr1e-3\", \"hidden\":[256], \"lr\":1e-3},\n",
        "    {\"name\":\"mlp_256_lr5e-4\", \"hidden\":[256], \"lr\":5e-4},\n",
        "]\n",
        "\n",
        "def cv_score(config):\n",
        "    fold_scores = []\n",
        "    t0 = time.perf_counter()\n",
        "    for k,(tr,va) in enumerate(skf.split(X_all, y_all),1):\n",
        "        fk = time.perf_counter()\n",
        "        f1, _, _ = train_one_fold(X_all[tr], y_all[tr], X_all[va], y_all[va],\n",
        "                                  config, max_epochs=120, patience=12, bs=2048)\n",
        "        fold_scores.append(f1)\n",
        "        print(f\"{config['name']} | fold {k}/5  F1-macro={f1:.4f} | elapsed={time.perf_counter()-fk:.1f}s\")\n",
        "    print(f\"→ {config['name']} total={time.perf_counter()-t0:.1f}s\\n\")\n",
        "    return np.array(fold_scores)\n",
        "\n",
        "# roda grid\n",
        "grid_results = []\n",
        "for cfg in param_grid:\n",
        "    scores = cv_score(cfg)\n",
        "    grid_results.append({\"config\":cfg, \"mean\":scores.mean(), \"std\":scores.std(), \"per_fold\":scores.tolist()})\n",
        "grid_results = sorted(grid_results, key=lambda d: d[\"mean\"], reverse=True)\n",
        "\n",
        "best = grid_results[0]\n",
        "best_config = best[\"config\"]\n",
        "print(\"\\n>>> MELHOR CONFIG:\", best_config, f\"| F1-macro CV = {best['mean']:.4f} ± {best['std']:.4f}\")\n",
        "print(\"Resultados por fold (F1-macro):\", np.round(best[\"per_fold\"], 4))\n",
        "\n",
        "# OOF: treina por fold e grava predições nas posições corretas\n",
        "oof_pred = np.empty_like(y_all)\n",
        "fold_models, fold_scalers = [], []\n",
        "for k,(tr,va) in enumerate(skf.split(X_all, y_all),1):\n",
        "    f1, model, scaler = train_one_fold(X_all[tr], y_all[tr], X_all[va], y_all[va],\n",
        "                                       best_config, max_epochs=120, patience=12, bs=2048)\n",
        "    Xv = scaler.transform(X_all[va]).astype(np.float32)\n",
        "    with torch.no_grad():\n",
        "        with AMP():\n",
        "            logits = model(torch.from_numpy(Xv)).argmax(1).numpy()\n",
        "    oof_pred[va] = logits\n",
        "    fold_models.append(model); fold_scalers.append(scaler)\n",
        "    print(f\"[OOF] fold {k}: F1-macro={f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report (OOF):\")\n",
        "print(classification_report(y_all, oof_pred, digits=4))\n",
        "cm = confusion_matrix(y_all, oof_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{i}\" for i in range(n_classes)],\n",
        "                     columns=[f\"pred_{i}\" for i in range(n_classes)])\n",
        "cm_df.head()\n",
        "\n",
        "# salva um modelo\n",
        "final_scaler = StandardScaler().fit(X_all)\n",
        "X_full = final_scaler.transform(X_all).astype(np.float32)\n",
        "full_loader = DataLoader(TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_all)),\n",
        "                         batch_size=4096, shuffle=True)\n",
        "\n",
        "final = MLP(n_features, best_config[\"hidden\"], n_classes, dropout=0.2).to(DEVICE)\n",
        "opt = torch.optim.AdamW(final.parameters(), lr=best_config[\"lr\"], weight_decay=1e-4)\n",
        "scaler_amp = make_scaler()\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "for ep in range(20):  # treino curto só pra materializar o artefato\n",
        "    final.train()\n",
        "    for xb,yb in full_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with AMP():\n",
        "            loss = crit(final(xb), yb)\n",
        "        if scaler_amp is not None:\n",
        "            scaler_amp.scale(loss).backward(); scaler_amp.step(opt); scaler_amp.update()\n",
        "        else:\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "os.makedirs(\"/content/models\", exist_ok=True)\n",
        "torch.save(final.state_dict(), \"/content/models/best_mlp.pth\")\n",
        "import joblib\n",
        "joblib.dump(final_scaler, \"/content/models/best_scaler.pkl\")\n",
        "with open(\"/content/models/best_cv_summary.json\",\"w\") as f:\n",
        "    json.dump(best, f, indent=2)\n",
        "\n",
        "print(\"\\nArtefatos salvos em /content/models/: best_mlp.pth, best_scaler.pkl, best_cv_summary.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melhor solução.\n",
        "Após busca em grade com validação cruzada 5-fold, o modelo vencedor foi um MLP (256) com lr=1e-3. O desempenho médio foi F1-macro = 0.6759 ± 0.0197 (métricas por fold mostradas acima). O relatório OOF e a matriz de confusão confirmam generalização consistente entre as classes. O scaler e os pesos finais foram salvos para reuso."
      ],
      "metadata": {
        "id": "SuHDixmwBV_I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0sQ29RkAJBe"
      },
      "source": [
        "## Empacotando a solução\n",
        "\n",
        "Suponha que você deve entregar este classificador ao órgão responsável por administrar o Roosevelt National Park. Para tanto, você deve fazer uma preparação do mesmo para utilização neste cenário. Uma vez que já identificou os melhores parâmetros e hiperparâmetros, o passo remanescente consiste em treinar o modelo com estes valores e todos os dados disponíveis, salvando o conjunto de pesos do modelo ao final para entrega ao cliente. Assim, finalize o projeto prático realizando tais passos.\n",
        "\n",
        "1. Consulte a documentação a seguir:\n",
        "https://scikit-learn.org/stable/modules/model_persistence.html  \n",
        "2. Treine o modelo com todos os dados  \n",
        "3. Salve o modelo em disco  \n",
        "4. Construa uma rotina que recupere o modelo em disco  \n",
        "5. Mostre que a rotina é funcional, fazendo previsões com todos os elementos do dataset e exibindo uma matriz de confusão das mesmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gifMdf1eAJBg"
      },
      "outputs": [],
      "source": [
        "# EMPACOTAR\n",
        "import os, json, time, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# 0) GPU & AMP compat\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE, \"| CUDA:\", torch.cuda.is_available())\n",
        "\n",
        "USE_CUDA = (DEVICE.type == \"cuda\")\n",
        "try:\n",
        "    from torch import amp\n",
        "    def AMP(): return amp.autocast(\"cuda\", dtype=torch.float16) if USE_CUDA else nullcontext()\n",
        "    def make_scaler(): return amp.GradScaler(\"cuda\") if USE_CUDA else None\n",
        "    print(\"AMP: torch.amp\")\n",
        "except Exception:\n",
        "    from torch.cuda.amp import autocast as legacy_autocast, GradScaler as LegacyGradScaler\n",
        "    def AMP(): return legacy_autocast(enabled=USE_CUDA, dtype=torch.float16)\n",
        "    def make_scaler(): return LegacyGradScaler(enabled=USE_CUDA)\n",
        "    print(\"AMP: legacy\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 1) best_config\n",
        "if \"best_config\" not in globals():\n",
        "    with open(\"/content/models/best_cv_summary.json\") as f:\n",
        "        best = json.load(f)\n",
        "    best_config = best[\"config\"]\n",
        "print(\"best_config:\", best_config)\n",
        "\n",
        "# 2) dados\n",
        "if \"X_all\" not in globals() or \"y_all\" not in globals():\n",
        "    from sklearn.datasets import fetch_covtype\n",
        "    df = fetch_covtype(as_frame=True).frame\n",
        "    y_all = df[\"Cover_Type\"].astype(int).to_numpy()\n",
        "    X_all = df.drop(columns=[\"Cover_Type\"]).to_numpy(dtype=np.float32)\n",
        "\n",
        "n_features = X_all.shape[1]\n",
        "# importante: usa número de classes distintas (não max+1)\n",
        "n_classes  = int(np.unique(y_all).shape[0])\n",
        "\n",
        "# 3) scaler + dataset\n",
        "print(\"Padronizando full…\")\n",
        "final_scaler = StandardScaler().fit(X_all)\n",
        "X_full = np.ascontiguousarray(final_scaler.transform(X_all).astype(np.float32))\n",
        "y_full = y_all.astype(int)\n",
        "\n",
        "BATCH_SIZE   = 8192 if USE_CUDA else 4096\n",
        "NUM_WORKERS  = 0\n",
        "EPOCHS_FINAL = min(best_config.get(\"epochs\", 20), 10)\n",
        "print(f\"BATCH={BATCH_SIZE} | EPOCHS={EPOCHS_FINAL} | workers={NUM_WORKERS}\")\n",
        "\n",
        "dl = DataLoader(\n",
        "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=USE_CUDA\n",
        ")\n",
        "\n",
        "# 4) modelo\n",
        "assert \"MLP\" in globals(), \"Defina a classe MLP antes de executar esta célula.\"\n",
        "model = MLP(n_features, best_config[\"hidden\"], n_classes, dropout=best_config.get(\"dropout\", 0.2)).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=best_config[\"lr\"], weight_decay=best_config.get(\"wd\", 1e-4))\n",
        "crit = nn.CrossEntropyLoss()\n",
        "scaler_amp = make_scaler()\n",
        "\n",
        "# 5) treino com logs por época\n",
        "t0 = time.perf_counter()\n",
        "for ep in range(1, EPOCHS_FINAL + 1):\n",
        "    model.train(); losses = []; t_ep = time.perf_counter()\n",
        "    for i, (xb, yb) in enumerate(dl, 1):\n",
        "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with AMP():\n",
        "            loss = crit(model(xb), yb)\n",
        "        if scaler_amp is not None:\n",
        "            scaler_amp.scale(loss).backward(); scaler_amp.step(opt); scaler_amp.update()\n",
        "        else:\n",
        "            loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "        if i % 25 == 0 or i == len(dl):\n",
        "            print(f\"\\r[final] ep {ep}/{EPOCHS_FINAL} | step {i}/{len(dl)} | loss {np.mean(losses):.4f}\", end=\"\")\n",
        "    print(f\" | {time.perf_counter() - t_ep:.1f}s\")\n",
        "\n",
        "print(f\"Tempo total: {time.perf_counter() - t0:.1f}s\")\n",
        "\n",
        "# 6) salvar\n",
        "os.makedirs(\"/content/models\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/models/final_mlp.pth\")\n",
        "joblib.dump(final_scaler, \"/content/models/final_scaler.pkl\")\n",
        "meta = {\"n_features\": int(n_features), \"n_classes\": int(n_classes), \"best_config\": best_config}\n",
        "with open(\"/content/models/final_metadata.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"Salvo: final_mlp.pth, final_scaler.pkl, final_metadata.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EMPACOTAR — verificação: carregar, prever full e gerar matriz de confusão (robusto a mismatch)\n",
        "import os, json, numpy as np, pandas as pd, torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "assert os.path.exists(\"/content/models/final_metadata.json\"), \"Rode a célula A primeiro.\"\n",
        "\n",
        "# metadata (pode estar com n_classes desatualizado; vamos confiar no checkpoint)\n",
        "with open(\"/content/models/final_metadata.json\") as f:\n",
        "    meta = json.load(f)\n",
        "best_config_loaded = meta[\"best_config\"]\n",
        "n_features_loaded  = meta[\"n_features\"]\n",
        "\n",
        "# carrega pesos\n",
        "state = torch.load(\"/content/models/final_mlp.pth\", map_location=\"cpu\")\n",
        "\n",
        "# infere out_dim pelo shape do último Linear no checkpoint\n",
        "# pega a última key que termina com \".weight\" (ordem preservada)\n",
        "last_w_key = [k for k in state.keys() if k.endswith(\".weight\")][-1]\n",
        "out_dim_ckpt = state[last_w_key].shape[0]\n",
        "\n",
        "# recria modelo com o out_dim do checkpoint e carrega pesos\n",
        "model_loaded = MLP(\n",
        "    n_features_loaded,\n",
        "    best_config_loaded[\"hidden\"],\n",
        "    out_dim_ckpt,\n",
        "    dropout=best_config_loaded.get(\"dropout\", 0.2),\n",
        ").to(\"cpu\")\n",
        "model_loaded.load_state_dict(state, strict=True)\n",
        "model_loaded.eval()\n",
        "\n",
        "# dados + scaler\n",
        "if \"X_all\" not in globals() or \"y_all\" not in globals():\n",
        "    from sklearn.datasets import fetch_covtype\n",
        "    df = fetch_covtype(as_frame=True).frame\n",
        "    y_all = df[\"Cover_Type\"].astype(int).to_numpy()\n",
        "    X_all = df.drop(columns=[\"Cover_Type\"]).to_numpy(dtype=np.float32)\n",
        "\n",
        "scaler_loaded = joblib.load(\"/content/models/final_scaler.pkl\")\n",
        "X_inf = np.ascontiguousarray(scaler_loaded.transform(X_all).astype(np.float32))\n",
        "\n",
        "# inferência em lotes\n",
        "BS_PRED = 8192\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(X_inf), BS_PRED):\n",
        "        xb = torch.from_numpy(X_inf[i:i+BS_PRED])\n",
        "        preds.append(model_loaded(xb).argmax(1).numpy())\n",
        "y_pred = np.concatenate(preds)\n",
        "\n",
        "print(\"Classification report (FULL):\")\n",
        "print(classification_report(y_all, y_pred, digits=4))\n",
        "\n",
        "# matriz de confusão com labels reais (inclui 0 se o modelo usar)\n",
        "labels = np.unique(np.concatenate([y_all, y_pred]))\n",
        "cm = confusion_matrix(y_all, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"true_{c}\" for c in labels],\n",
        "    columns=[f\"pred_{c}\" for c in labels],\n",
        ")\n",
        "display(cm_df)\n",
        "\n",
        "# salvar\n",
        "os.makedirs(\"/content/reports\", exist_ok=True)\n",
        "cm_path = \"/content/reports/confusion_matrix_full.csv\"\n",
        "cm_df.to_csv(cm_path, index=True)\n",
        "print(\"Matriz salva em:\", cm_path)\n",
        "\n",
        "# atualiza metadata para refletir o out_dim real do checkpoint\n",
        "if meta.get(\"n_classes\", None) != int(out_dim_ckpt):\n",
        "    meta[\"n_classes\"] = int(out_dim_ckpt)\n",
        "    with open(\"/content/models/final_metadata.json\", \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata atualizado: n_classes = {int(out_dim_ckpt)}\")\n"
      ],
      "metadata": {
        "id": "LGQD3AkH6wZE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}